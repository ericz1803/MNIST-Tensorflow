{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "#tqdm for the progress bar\n",
    "from tqdm import tqdm\n",
    "print(\"All modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the files can be found on Kaggle.\n",
    "\n",
    "https://www.kaggle.com/c/digit-recognizer/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load in data from kaggle\n",
    "train_amt=37000\n",
    "\n",
    "train_X=pd.read_csv(\"~/Downloads/train.csv\")\n",
    "#randomly shuffle it before splitting it into training and validation sets\n",
    "#np.random.shuffle(train_X)\n",
    "\n",
    "validation_X=train_X.loc[train_amt:]\n",
    "train_X=train_X.loc[:train_amt-1]\n",
    "train_y=train_X['label']\n",
    "validation_y=validation_X['label']\n",
    "del train_X['label']\n",
    "del validation_X['label']\n",
    "test_X=pd.read_csv(\"~/Downloads/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37000, 784) (37000,) (5000, 784) (5000,) (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "#convert to numpy arrays and normalize values between 0 and 1\n",
    "#normalizing allows the network to train better and converge faster\n",
    "train_X=np.array(train_X)/255\n",
    "train_y=np.array(train_y)\n",
    "validation_X=np.array(validation_X)/255\n",
    "validation_y=np.array(validation_y)\n",
    "print(train_X.shape, train_y.shape, validation_X.shape, validation_y.shape, test_X.shape)\n",
    "#test data\n",
    "test_X=np.array(test_X).astype(dtype='float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37000, 28, 28, 1) (5000, 28, 28, 1) (28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X=train_X.reshape([-1, 28, 28, 1])\n",
    "validation_X=validation_X.reshape([-1, 28, 28, 1])\n",
    "test_X=test_X.reshape([-1, 28, 28, 1])\n",
    "print(train_X.shape, validation_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37000, 10) (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "#convert to one-hot array\n",
    "train_y=np.array(pd.get_dummies(train_y))\n",
    "validation_y=np.array(pd.get_dummies(validation_y))\n",
    "print(train_y.shape, validation_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f9b5835128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show random number\n",
    "test_img=train_X[np.random.randint(0,train_amt-1)].reshape(28,28)\n",
    "\n",
    "plt.imshow(test_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast_3:0' shape=(5000, 10) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure everything is a float32\n",
    "tf.cast(train_X, tf.float32)\n",
    "tf.cast(train_y, tf.float32)\n",
    "tf.cast(validation_X, tf.float32)\n",
    "tf.cast(validation_y, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv format=height, width, filters\n",
    "#max pool format=side length\n",
    "conv_layer1=[3,3,32]\n",
    "conv_layer2=[3,3,64]\n",
    "max_pool1=2\n",
    "conv_layer3=[3,3,128]\n",
    "conv_layer4=[3,3,256]\n",
    "max_pool2=2\n",
    "#output=7x7\n",
    "nodes_fc1=1000\n",
    "nodes_fc2=500\n",
    "nodes_output=10\n",
    "\n",
    "batch_size=128\n",
    "num_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regularizer=tf.contrib.layers.l2_regularizer(scale=0.002)\n",
    "\n",
    "weights_conv_layer1=tf.get_variable('conv_weights_layer_1', dtype=tf.float32, \n",
    "                            initializer=tf.truncated_normal([conv_layer1[0], conv_layer1[1], 1, conv_layer1[2]], dtype=tf.float32, stddev=0.2),\n",
    "                                   regularizer=regularizer)\n",
    "biases_conv_layer1=tf.get_variable('conv_biases_layer_1', dtype=tf.float32, initializer= tf.zeros([conv_layer1[2]]))\n",
    "\n",
    "weights_conv_layer2=tf.get_variable('conv_weights_layer_2', dtype=tf.float32, \n",
    "                            initializer=tf.truncated_normal([conv_layer2[0], conv_layer2[1], conv_layer1[2], conv_layer2[2]], dtype=tf.float32, stddev=0.2),\n",
    "                                   regularizer=regularizer)\n",
    "\n",
    "biases_conv_layer2=tf.get_variable('conv_biases_layer_2', dtype=tf.float32, initializer= tf.zeros([conv_layer2[2]]))\n",
    "\n",
    "weights_conv_layer3=tf.get_variable('conv_weights_layer_3', dtype=tf.float32, \n",
    "                            initializer=tf.truncated_normal([conv_layer3[0], conv_layer3[1], conv_layer2[2], conv_layer3[2]], dtype=tf.float32, stddev=0.2),\n",
    "                                   regularizer=regularizer)\n",
    "biases_conv_layer3=tf.get_variable('conv_biases_layer_3', dtype=tf.float32, initializer= tf.zeros([conv_layer3[2]]))\n",
    "\n",
    "weights_conv_layer4=tf.get_variable('conv_weights_layer_4', dtype=tf.float32, \n",
    "                            initializer=tf.truncated_normal([conv_layer4[0], conv_layer4[1], conv_layer3[2], conv_layer4[2]], dtype=tf.float32, stddev=0.2),\n",
    "                                   regularizer=regularizer)\n",
    "biases_conv_layer4=tf.get_variable('conv_biases_layer_4', dtype=tf.float32, initializer= tf.zeros([conv_layer4[2]]))\n",
    "\n",
    "weights_fc1=tf.get_variable('weights_fc1', dtype=tf.float32, \n",
    "                            initializer=tf.truncated_normal([49*conv_layer4[2], nodes_fc1], dtype=tf.float32, stddev=np.sqrt(2/nodes_fc1)),\n",
    "                           regularizer=regularizer)\n",
    "biases_fc1=tf.get_variable('biases_fc1', dtype=tf.float32, initializer=tf.zeros([nodes_fc1]))\n",
    "\n",
    "weights_fc2=tf.get_variable('weights_fc2', dtype=tf.float32, \n",
    "                            initializer=tf.truncated_normal([nodes_fc1, nodes_fc2], dtype=tf.float32, stddev=np.sqrt(2/nodes_fc2)),\n",
    "                           regularizer=regularizer)\n",
    "biases_fc2=tf.get_variable('biases_fc2', dtype=tf.float32, initializer=tf.zeros([nodes_fc2]))\n",
    "\n",
    "weights_output=tf.get_variable('weights_output', dtype=tf.float32, \n",
    "                               initializer=tf.truncated_normal([nodes_fc2, nodes_output], dtype=tf.float32, stddev=np.sqrt(2/nodes_output)),\n",
    "                              regularizer=regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(layer_input, layer_weights, layer_biases, is_training, strides=1):\n",
    "    conv_layer=tf.nn.conv2d(layer_input, layer_weights, [1, strides, strides, 1], 'SAME')\n",
    "    conv_layer=tf.nn.bias_add(conv_layer, layer_biases)\n",
    "    #batch normalization\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training = is_training)\n",
    "    conv_layer=tf.nn.elu(conv_layer)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(layer, width):\n",
    "    return tf.nn.max_pool(value=layer, ksize=[1, width, width, 1], strides=[1, width, width, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting up placeholders where data will be passed into  later\n",
    "features=tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "labels=tf.placeholder(tf.float32)\n",
    "train=tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create saver, max_to_keep is maximum checkpoint files kept\n",
    "saver=tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, train):\n",
    "    #train is a bool\n",
    "    #set conv keep to a number if train is true, else set it to 1\n",
    "    conv_keep=0.8 if train is not None else 1.0\n",
    "    fc_keep=0.7 if train is not None else 1.0\n",
    "    \n",
    "    conv_layer1=conv_layer(x, weights_conv_layer1, biases_conv_layer1, train)\n",
    "    conv_layer1=tf.nn.dropout(conv_layer1, conv_keep)\n",
    "    \n",
    "    conv_layer2=conv_layer(conv_layer1, weights_conv_layer2, biases_conv_layer2, train)\n",
    "    conv_layer2=tf.nn.dropout(conv_layer2, conv_keep)\n",
    "    \n",
    "    max_pool_1=max_pool(conv_layer2, max_pool1)\n",
    "    \n",
    "    conv_layer3=conv_layer(max_pool_1, weights_conv_layer3, biases_conv_layer3, train)\n",
    "    conv_layer3=tf.nn.dropout(conv_layer3, conv_keep)\n",
    "    \n",
    "    conv_layer4=conv_layer(conv_layer3, weights_conv_layer4, biases_conv_layer4, train)\n",
    "    conv_layer4=tf.nn.dropout(conv_layer4, conv_keep)\n",
    "    \n",
    "    max_pool_2=max_pool(conv_layer4, max_pool2)\n",
    "    \n",
    "    fc_input=tf.contrib.layers.flatten(max_pool_2)\n",
    "    \n",
    "    fc1=tf.add(tf.matmul(fc_input, weights_fc1), biases_fc1)\n",
    "    #batch norm\n",
    "    fc1=tf.layers.batch_normalization(fc1, training = train)\n",
    "    fc1=tf.nn.dropout(fc1, fc_keep)\n",
    "    fc1=tf.nn.elu(fc1)\n",
    "    \n",
    "    fc2=tf.add(tf.matmul(fc1, weights_fc2), biases_fc2)\n",
    "    fc2=tf.layers.batch_normalization(fc2, training = train)\n",
    "    fc2=tf.nn.dropout(fc2, fc_keep)\n",
    "    fc2=tf.nn.elu(fc2)\n",
    "    \n",
    "    logits=tf.matmul(fc2, weights_output)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cost and gradient descent\n",
    "#tf.reduce_mean=np.mean and tf.reduce_sum=np.sum\n",
    "lr=1e-2\n",
    "learning_rate=tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "logits=forward_pass(features,train)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#accuracy\n",
    "#argmax takes the maximum value in each vector and sets it to 1, all others are set to 0\n",
    "output=tf.nn.softmax(logits)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1)),tf.float32))\n",
    "\n",
    "#used later for predicting the test data\n",
    "prediction=tf.argmax(tf.nn.softmax(logits=output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:35<00:00,  8.26batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 94.68707298691152, Validation Accuracy = 0.9453125\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0 to 0.9453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.69batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 36.30836476298282, Validation Accuracy = 0.9796875\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.9453125 to 0.9796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.63batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 18.143628604258993, Validation Accuracy = 0.9890625\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.9796875 to 0.9890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.76batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 14.72338069230318, Validation Accuracy = 0.9703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.74batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 12.649944556178525, Validation Accuracy = 0.98125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.66batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 11.4087734515997, Validation Accuracy = 0.9828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.75batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 10.221424190196558, Validation Accuracy = 0.9734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.74batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 15.922715500684717, Validation Accuracy = 0.9859375\n",
      "Learning rate decreased to 0.005.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.75batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 5.807273441518191, Validation Accuracy = 0.990625\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.9890625 to 0.990625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.75batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 6.158489199195174, Validation Accuracy = 0.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.68batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 6.150204072444467, Validation Accuracy = 0.9859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.78batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 3.9284187627927167, Validation Accuracy = 0.9859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.78batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 6.640222938258374, Validation Accuracy = 0.9890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.71batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 3.296627636944322, Validation Accuracy = 0.9828125\n",
      "Learning rate decreased to 0.0025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.75batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 2.1831471326513565, Validation Accuracy = 0.990625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.75batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 2.526394499186722, Validation Accuracy = 0.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.76batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 1.7379512047955359, Validation Accuracy = 0.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.75batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 2.6533792337722844, Validation Accuracy = 0.990625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.75batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 1.9781479279536143, Validation Accuracy = 0.990625\n",
      "Learning rate decreased to 0.00125.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:32<00:00,  8.79batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 1.2166584597028418, Validation Accuracy = 0.9890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:32<00:00,  8.82batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 1.3730863210075768, Validation Accuracy = 0.9859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.76batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 1.303329743620992, Validation Accuracy = 0.9859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.78batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 0.9596357564969367, Validation Accuracy = 0.990625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:33<00:00,  8.78batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 0.6149614761234261, Validation Accuracy = 0.990625\n",
      "Learning rate decreased to 0.000625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████████████████████████████████████████████████████████| 290/290 [00:32<00:00,  8.79batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 0.5749583488886856, Validation Accuracy = 0.9921875\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.990625 to 0.9921875\n",
      "Training Finished! It took 15.0 minutes.\n",
      " Best validation accuracy: 0.9921875\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Best Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|███████████████████████████████████████████████████| 219/219 [00:09<00:00, 23.63batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000,)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "before_time=time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #restore weights if file found\n",
    "    try:\n",
    "        saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "        print(\"Model restored.\")\n",
    "    except:\n",
    "        print(\"No save file found.\")\n",
    "\n",
    "    \n",
    "    batch_count = int(math.ceil(len(train_X)/batch_size))\n",
    "    val_batches= int(math.ceil(len(validation_X)/1000))\n",
    "    best_val_acc=0\n",
    "    last_improve_epoch=0\n",
    "    for epoch in range(num_epochs):\n",
    "        #shuffle data\n",
    "        state=np.random.get_state()\n",
    "        np.random.shuffle(train_X)\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(train_y)\n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch+1, num_epochs), unit='batches')\n",
    "        train_loss=0.0\n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_X[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_y[batch_start:batch_start + batch_size]\n",
    "            #train\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={features: batch_features, labels: batch_labels, \n",
    "                                                          learning_rate: lr, train: True})\n",
    "            train_loss+=c\n",
    "        #set keep amount to 100% for testing\n",
    "        validation_accuracy=0\n",
    "        for batch_i in range(val_batches):\n",
    "            batch_start = batch_i*batch_size\n",
    "            validation_accuracy+=sess.run(accuracy, \n",
    "                                          feed_dict={features: validation_X[batch_start:batch_start+batch_size],\n",
    "                                                     labels: validation_y[batch_start:batch_start+batch_size],\n",
    "                                                     train: False})\n",
    "        validation_accuracy/=val_batches\n",
    "        print('Training Loss = {}, Validation Accuracy = {}'.format(train_loss, validation_accuracy))\n",
    "\n",
    "        #save model if validation is at a new best\n",
    "        if validation_accuracy>best_val_acc:\n",
    "            save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "            print(\"Model saved in file: {}\".format(save_path))\n",
    "            print(\"Accuracy improved from {} to {}\".format(best_val_acc, validation_accuracy))\n",
    "            best_val_acc=validation_accuracy\n",
    "            last_improve_epoch=epoch\n",
    "        #if model hasn't improved for 5 epochs step down learning rate\n",
    "        elif (epoch-last_improve_epoch)%5==0:\n",
    "            lr=max(lr/2,1e-6)\n",
    "            print(\"Learning rate decreased to {}.\".format(lr))\n",
    "        #stop training if validation loss hasn't improved for 10 epoch\n",
    "        \n",
    "    print(\"Training Finished! It took {} minutes.\\n Best validation accuracy: {}\"\n",
    "          .format(np.round((time.time()-before_time)/60,2), best_val_acc))\n",
    "    \n",
    "    #load in best model\n",
    "    try:\n",
    "        saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "        print(\"Best Model restored.\")\n",
    "    except:\n",
    "        print(\"No save file found. Prediction will use current weights which may not be the best.\")\n",
    "       \n",
    "        \n",
    "    predictions=np.array([])\n",
    "    batches_test = int(math.ceil(len(test_X)/batch_size))\n",
    "    progress_bar = tqdm(range(batches_test), desc='Generating Predictions', unit='batches')\n",
    "    for batch_i in progress_bar:\n",
    "        # Get a batch of test features and labels\n",
    "        batch_start = batch_i*batch_size\n",
    "        batch_features = test_X[batch_start:batch_start + batch_size]\n",
    "        predictions=np.append(predictions, sess.run(prediction, feed_dict={features: batch_features, train: False}))\n",
    "    print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use replace the 0s in the sample submission file with the outputs from the neural net\n",
    "submission=pd.read_csv(\"~/Downloads/sample_submission.csv\")\n",
    "for x in range(len(predictions)):\n",
    "    submission['Label'][x]+=predictions[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ImageId  Label\n",
      "0            1      2\n",
      "1            2      0\n",
      "2            3      9\n",
      "3            4      0\n",
      "4            5      3\n",
      "5            6      7\n",
      "6            7      0\n",
      "7            8      3\n",
      "8            9      0\n",
      "9           10      3\n",
      "10          11      5\n",
      "11          12      7\n",
      "12          13      4\n",
      "13          14      0\n",
      "14          15      4\n",
      "15          16      3\n",
      "16          17      3\n",
      "17          18      1\n",
      "18          19      9\n",
      "19          20      0\n",
      "20          21      9\n",
      "21          22      1\n",
      "22          23      1\n",
      "23          24      5\n",
      "24          25      7\n",
      "25          26      4\n",
      "26          27      2\n",
      "27          28      7\n",
      "28          29      4\n",
      "29          30      7\n",
      "...        ...    ...\n",
      "27970    27971      5\n",
      "27971    27972      0\n",
      "27972    27973      4\n",
      "27973    27974      8\n",
      "27974    27975      0\n",
      "27975    27976      3\n",
      "27976    27977      6\n",
      "27977    27978      0\n",
      "27978    27979      1\n",
      "27979    27980      9\n",
      "27980    27981      3\n",
      "27981    27982      1\n",
      "27982    27983      1\n",
      "27983    27984      0\n",
      "27984    27985      4\n",
      "27985    27986      5\n",
      "27986    27987      2\n",
      "27987    27988      2\n",
      "27988    27989      9\n",
      "27989    27990      6\n",
      "27990    27991      7\n",
      "27991    27992      6\n",
      "27992    27993      1\n",
      "27993    27994      9\n",
      "27994    27995      7\n",
      "27995    27996      9\n",
      "27996    27997      7\n",
      "27997    27998      3\n",
      "27998    27999      9\n",
      "27999    28000      2\n",
      "\n",
      "[28000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#index=False gets rid of the double numbers\n",
    "submission.to_csv(\"~/Downloads/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
