{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "#tqdm for the progress bar\n",
    "from tqdm import tqdm\n",
    "print(\"All modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the files can be found on Kaggle.\n",
    "\n",
    "https://www.kaggle.com/c/digit-recognizer/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load in data from kaggle\n",
    "train_X=pd.read_csv(\"C:/Users/Eric Zhou/Downloads/train.csv\")\n",
    "validation_X=train_X.loc[40000:]\n",
    "train_X=train_X.loc[:39999]\n",
    "train_y=train_X['label']\n",
    "validation_y=validation_X['label']\n",
    "del train_X['label']\n",
    "del validation_X['label']\n",
    "test_X=pd.read_csv(\"C:/Users/Eric Zhou/Downloads/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 784) (40000,) (2000, 784) (2000,)\n"
     ]
    }
   ],
   "source": [
    "#convert to numpy arrays and normalize values between 0 and 1\n",
    "#normalizing allows the network to train better and converge faster\n",
    "train_X=np.array(train_X)/255\n",
    "train_y=np.array(train_y)\n",
    "validation_X=np.array(validation_X)/255\n",
    "validation_y=np.array(validation_y)\n",
    "print(train_X.shape, train_y.shape, validation_X.shape, validation_y.shape)\n",
    "#test data\n",
    "test_X=np.array(test_X).astype(dtype='float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "#convert to one-hot array\n",
    "train_y=np.array(pd.get_dummies(train_y))\n",
    "validation_y=np.array(pd.get_dummies(validation_y))\n",
    "print(train_y.shape, validation_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast_3:0' shape=(2000, 10) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure everything is a float32\n",
    "tf.cast(train_X, tf.float32)\n",
    "tf.cast(train_y, tf.float32)\n",
    "tf.cast(validation_X, tf.float32)\n",
    "tf.cast(validation_y, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting up placeholders where data will be passed into  later\n",
    "features=tf.placeholder(tf.float32, shape=[None, 784])\n",
    "labels=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set some parameters\n",
    "batch_size=128\n",
    "\n",
    "nodes_hl1=1000\n",
    "nodes_hl2=500\n",
    "nodes_hl3=100\n",
    "\n",
    "output_size=10\n",
    "\n",
    "num_epochs=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A website showing different weight initializations:\n",
    "https://intoli.com/blog/neural-network-initialization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create variables(weights and biases) Uses standard deviation of sqrt(2/nodes) which is a good starting point.\n",
    "\n",
    "weights_input_hl1=tf.get_variable('weights_input_hl1', dtype=tf.float32, \n",
    "  initializer=tf.truncated_normal([784, nodes_hl1], dtype=tf.float32, stddev=np.sqrt(2/784)))\n",
    "biases_hl1=tf.get_variable('biases_hl1', [nodes_hl1], dtype=tf.float32, \n",
    "  initializer=tf.zeros_initializer)\n",
    "\n",
    "weights_hl1_hl2=tf.get_variable('weights_hl1_hl2', dtype=tf.float32, \n",
    "  initializer=tf.truncated_normal([nodes_hl1, nodes_hl2], dtype=tf.float32, stddev=np.sqrt(2/nodes_hl1)))\n",
    "biases_hl2=tf.get_variable('biases_hl2', [nodes_hl2], dtype=tf.float32, \n",
    "  initializer=tf.zeros_initializer)\n",
    "\n",
    "weights_hl2_hl3=tf.get_variable('weights_hl2_hl3', dtype=tf.float32, \n",
    "  initializer=tf.truncated_normal([nodes_hl2, nodes_hl3], dtype=tf.float32, stddev=np.sqrt(2/nodes_hl2)))\n",
    "biases_hl3=tf.get_variable('biases_hl3', [nodes_hl3], dtype=tf.float32, \n",
    "  initializer=tf.zeros_initializer)\n",
    "\n",
    "weights_hl3_output=tf.get_variable('weights_hl3_output', dtype=tf.float32, \n",
    "  initializer=tf.truncated_normal([nodes_hl3, output_size], dtype=tf.float32, stddev=np.sqrt(2/nodes_hl3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create saver, max_to_keep is maximum checkpoint files kept\n",
    "saver=tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropout rate, each time it is trained, ~20% of neurons will be killed in each layer, it helps prevent overfitting\n",
    "train_keep=0.8\n",
    "keep_amt=train_keep\n",
    "\n",
    "#training pass\n",
    "#elu=exponential linear unit, generally performs better than relu\n",
    "\n",
    "def forward_pass(x, keep_amt):\n",
    "    dropout_rate=tf.constant(keep_amt)\n",
    "    l1=tf.add(tf.matmul(x, weights_input_hl1), biases_hl1)\n",
    "    l1=tf.nn.elu(l1)\n",
    "    l1=tf.nn.dropout(l1, dropout_rate)\n",
    "    l2=tf.add(tf.matmul(l1, weights_hl1_hl2), biases_hl2)\n",
    "    l2=tf.nn.elu(l2)\n",
    "    l2=tf.nn.dropout(l2, dropout_rate)\n",
    "    l3=tf.add(tf.matmul(l2, weights_hl2_hl3), biases_hl3)\n",
    "    l3=tf.nn.elu(l3)\n",
    "    l3=tf.nn.dropout(l3, dropout_rate)\n",
    "    output_layer=tf.matmul(l3, weights_hl3_output)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cost and gradient descent\n",
    "#tf.reduce_mean=np.mean and tf.reduce_sum=np.sum\n",
    "lr=1e-3\n",
    "learning_rate=tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "logits=forward_pass(features,keep_amt)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#accuracy\n",
    "#argmax takes the maximum value in each vector and sets it to 1, all others are set to 0\n",
    "output=tf.nn.softmax(logits)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1)),tf.float32))\n",
    "\n",
    "#used later for predicting the test data\n",
    "prediction=tf.argmax(tf.nn.softmax(logits=output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "No save file found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.00batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 102.3697184920311, Validation Accuracy = 0.9319999814033508\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0 to 0.9319999814033508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.49batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 53.88555122539401, Validation Accuracy = 0.9459999799728394\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.9319999814033508 to 0.9459999799728394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 36.17batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 41.96949165314436, Validation Accuracy = 0.9599999785423279\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.9459999799728394 to 0.9599999785423279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 36.19batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 34.636224480345845, Validation Accuracy = 0.9574999809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 36.10batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 28.119031809270382, Validation Accuracy = 0.9434999823570251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.57batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 25.290360514074564, Validation Accuracy = 0.9629999995231628\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.9599999785423279 to 0.9629999995231628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.38batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 24.09494905732572, Validation Accuracy = 0.9585000276565552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.27batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 20.123904526233673, Validation Accuracy = 0.9664999842643738\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.9629999995231628 to 0.9664999842643738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.98batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 19.799405670259148, Validation Accuracy = 0.9639999866485596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 36.13batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 17.159060012549162, Validation Accuracy = 0.9674999713897705\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.9664999842643738 to 0.9674999713897705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.53batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 16.082715121563524, Validation Accuracy = 0.9729999899864197\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.9674999713897705 to 0.9729999899864197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 34.98batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 14.706055641639978, Validation Accuracy = 0.9700000286102295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:09<00:00, 32.78batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 13.086639221408404, Validation Accuracy = 0.9605000019073486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.66batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 14.270017648465, Validation Accuracy = 0.9645000100135803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 36.08batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 13.228970643191133, Validation Accuracy = 0.9649999737739563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.36batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 13.55996012617834, Validation Accuracy = 0.9614999890327454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.47batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 6.082643291156273, Validation Accuracy = 0.9804999828338623\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Accuracy improved from 0.9729999899864197 to 0.9804999828338623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.43batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 4.013365510996664, Validation Accuracy = 0.9764999747276306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.46batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 3.4345639409439173, Validation Accuracy = 0.9754999876022339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.20batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 3.0340399424894713, Validation Accuracy = 0.9750000238418579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.66batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 2.4906575869536027, Validation Accuracy = 0.9800000190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.17batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 2.532156542278244, Validation Accuracy = 0.9785000085830688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.08batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 2.4492944907105993, Validation Accuracy = 0.9794999957084656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.64batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 2.1419501418859, Validation Accuracy = 0.9789999723434448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 36.04batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 2.0511268072295934, Validation Accuracy = 0.9764999747276306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.30batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 1.863662065457902, Validation Accuracy = 0.9800000190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200: 100%|█████████████████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.66batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss = 2.1856867607857566, Validation Accuracy = 0.9775000214576721\n",
      "Model has not improved for 10 epochs. Training has been stopped.\n",
      "Best validation accuracy: 0.9804999828338623\n",
      "Training Finished! It took 4.79 minutes.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Best Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|███████████████████████████████████████████████████| 219/219 [00:02<00:00, 91.58batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000,)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "before_time=time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #restore weights if file found\n",
    "    try:\n",
    "        saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "        print(\"Model restored.\")\n",
    "    except:\n",
    "        print(\"No save file found.\")\n",
    "\n",
    "    \n",
    "    batch_count = int(math.ceil(len(train_X)/batch_size))\n",
    "    best_val_acc=0\n",
    "    last_improve_epoch=0\n",
    "    for epoch in range(num_epochs):\n",
    "        #shuffle data\n",
    "        state=np.random.get_state()\n",
    "        np.random.shuffle(train_X)\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(train_y)\n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch+1, num_epochs), unit='batches')\n",
    "        train_loss=0.0\n",
    "        # The training cycle\n",
    "        keep_amt=train_keep\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_X[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_y[batch_start:batch_start + batch_size]\n",
    "            #train\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={features: batch_features, labels: batch_labels, learning_rate:lr})\n",
    "            train_loss+=c\n",
    "        #set keep amount to 100% for testing\n",
    "        keep_amt=1.0    \n",
    "        validation_accuracy=sess.run(accuracy, feed_dict={features: validation_X, labels: validation_y})\n",
    "        print('Training Loss = {}, Validation Accuracy = {}'.format(train_loss, validation_accuracy))\n",
    "\n",
    "        #save model if validation is at a new best and do not save for first 5 epochs\n",
    "        if validation_accuracy>best_val_acc:\n",
    "            save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "            print(\"Model saved in file: {}\".format(save_path))\n",
    "            print(\"Accuracy improved from {} to {}\".format(best_val_acc, validation_accuracy))\n",
    "            best_val_acc=validation_accuracy\n",
    "            last_improve_epoch=epoch\n",
    "        #if model hasn't improved for 5 epochs step down learning rate\n",
    "        elif (epoch-last_improve_epoch)%5==0:\n",
    "            lr/=10\n",
    "            print(\"Learning rate decreased.\")\n",
    "        #stop training if validation loss hasn't improved for 10 epochs \n",
    "        if epoch>=last_improve_epoch+10:\n",
    "            print(\"Model has not improved for 10 epochs. Training has been stopped.\")\n",
    "            print(\"Best validation accuracy: {}\".format(best_val_acc))\n",
    "            break;\n",
    "    print(\"Training Finished! It took {} minutes.\".format(np.round((time.time()-before_time)/60,2)))\n",
    "    \n",
    "    #load in best model\n",
    "    try:\n",
    "        saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "        print(\"Best Model restored.\")\n",
    "    except:\n",
    "        print(\"No save file found. Prediction will use current weights which may not be the best.\")\n",
    "\n",
    "    keep_amt=1.0\n",
    "    predictions=np.array([])\n",
    "    batches_test = int(math.ceil(len(test_X)/batch_size))\n",
    "    progress_bar = tqdm(range(batches_test), desc='Generating Predictions', unit='batches')\n",
    "    for batch_i in progress_bar:\n",
    "        # Get a batch of test features and labels\n",
    "        batch_start = batch_i*batch_size\n",
    "        batch_features = test_X[batch_start:batch_start + batch_size]\n",
    "        predictions=np.append(predictions, sess.run(prediction, feed_dict={features: batch_features}))\n",
    "    print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  0.,  9., ...,  3.,  9.,  2.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use replace the 0s in the sample submission file with the outputs from the neural net\n",
    "submission=pd.read_csv(\"~/Downloads/sample_submission.csv\")\n",
    "for x in range(len(predictions)):\n",
    "    submission['Label'][x]+=predictions[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ImageId  Label\n",
      "0            1      2\n",
      "1            2      0\n",
      "2            3      9\n",
      "3            4      9\n",
      "4            5      3\n",
      "5            6      7\n",
      "6            7      0\n",
      "7            8      3\n",
      "8            9      0\n",
      "9           10      3\n",
      "10          11      5\n",
      "11          12      7\n",
      "12          13      4\n",
      "13          14      0\n",
      "14          15      4\n",
      "15          16      3\n",
      "16          17      3\n",
      "17          18      1\n",
      "18          19      9\n",
      "19          20      0\n",
      "20          21      9\n",
      "21          22      1\n",
      "22          23      1\n",
      "23          24      5\n",
      "24          25      7\n",
      "25          26      4\n",
      "26          27      2\n",
      "27          28      7\n",
      "28          29      4\n",
      "29          30      7\n",
      "...        ...    ...\n",
      "27970    27971      5\n",
      "27971    27972      0\n",
      "27972    27973      4\n",
      "27973    27974      8\n",
      "27974    27975      0\n",
      "27975    27976      3\n",
      "27976    27977      6\n",
      "27977    27978      0\n",
      "27978    27979      1\n",
      "27979    27980      9\n",
      "27980    27981      3\n",
      "27981    27982      1\n",
      "27982    27983      1\n",
      "27983    27984      0\n",
      "27984    27985      4\n",
      "27985    27986      5\n",
      "27986    27987      2\n",
      "27987    27988      2\n",
      "27988    27989      9\n",
      "27989    27990      6\n",
      "27990    27991      7\n",
      "27991    27992      6\n",
      "27992    27993      1\n",
      "27993    27994      9\n",
      "27994    27995      7\n",
      "27995    27996      9\n",
      "27996    27997      7\n",
      "27997    27998      3\n",
      "27998    27999      9\n",
      "27999    28000      2\n",
      "\n",
      "[28000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#index=False gets rid of the double numbers\n",
    "submission.to_csv(\"~/Downloads/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
